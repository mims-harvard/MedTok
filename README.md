<h1 align="center">
  MedTok: Multimodal Medical Code Tokenizer
</h1>

## üëÄ Overview of MedTok
Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.

![MedTok framework](https://github.com/mims-harvard/MedTok/blob/main/MedTok.jpg)

## üöÄ Installation

1‚É£Ô∏è First, clone the Github repository:

```bash
git clone https://github.com/mims-harvard/MedTok
cd MedTok
```

2‚É£Ô∏è Then, set up the environment. To create an environment with all of the required packages, please ensure that conda is installed and then execute the commands:

```bash
conda env create -f MedTok.yaml
conda activate MedTok
```

## üí° How to train MedTok?

After cloning the repository and installing all dependencies. If you want to train MedTok with your customized parameter combinations or your customized encoders. Please execute the command:

```bash
python train.py
```

## üõ†Ô∏è How to use MedTok?
MedTok is a medical code tokenizer. When you are trying to use MedTok to tokenize a medical code, please just run the following command:

```bash
python inference.py
```
[TODO] need to pack it as hugging face model, will be finished today

### MedTok with QA Task
MedTok can be generalized to medicalQA systems, where the generated tokens are as prefix tokens to make LLMs answer input questions more accurately. To achieve this, we finetune LLMs with tokens generated by MedTok.

To finetune LLMs with datasets we presented in our paper, please run the following command:
```bash
WORLD_SIZE=1 CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --master_port 1234 fintune_llama3.py
```
After obtaining the pre-trained model, please do inference directly on other datasets:
```bash
WORLD_SIZE=1 CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --master_port 1234 inference.py
```

If you want to apply MedTok to your own QA system or datasets, please first extract the diseases contained in each query and obtain their medical code, and then prepare the datasets to be used as training dataset to finetune LLMs.
```bash
python extract_disease.py
python map_query_id.py
```

## Citation
```bash
@article{su2025multimodal,
  title={Multimodal Medical Code Tokenizer},
  author={Su, Xiaorui and Messica, Shvat and Huang, Yepeng and Johnson, Ruth and Fesser, Lukas and Gao, Shanghua and Sahneh, Faryad and Zitnik, Marinka},
  journal={International Conference on Machine Learning, ICML},
  year={2025}
}
```
</details>

